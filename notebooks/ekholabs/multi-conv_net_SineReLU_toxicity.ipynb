{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Convolutional Net for Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Conv Net performs sentiment analysis on the Google toxicity dataset review dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing import text\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, concatenate\n",
    "from keras.layers import Dense, Flatten, Dropout, Activation, BatchNormalization\n",
    "from keras.layers import Embedding, Conv1D, SpatialDropout1D, GlobalMaxPool1D, LSTM\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from keras_contrib.layers.advanced_activations import SineReLU\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'model_output/multi-conv'\n",
    "\n",
    "e_param = 0.05\n",
    "\n",
    "n_classes = 6\n",
    "\n",
    "epochs = 3\n",
    "patience = 1\n",
    "batch_size = 128\n",
    "test_split=.3\n",
    "\n",
    "n_dim = 128\n",
    "n_unique_words = 20000\n",
    "max_review_length = 400\n",
    "pad_type = trunc_type = 'pre'\n",
    "\n",
    "n_conv_1 = 32\n",
    "n_conv_2 = 64\n",
    "n_conv_3 = 128\n",
    "k_conv_1 = 2\n",
    "k_conv_2 = 4\n",
    "k_conv_3 = 5\n",
    "drop_conv = 0.5\n",
    "\n",
    "n_dense = 512\n",
    "dropout = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('kaggle/datasets/toxicity/train.csv')\n",
    "test_df = pd.read_csv('kaggle/datasets/toxicity/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences_series = train_df['comment_text'].fillna(\"_\").values\n",
    "test_sentences_series = test_df['comment_text'].fillna(\"_\").values\n",
    "\n",
    "# Tokeninze the Training data\n",
    "tokenizer = text.Tokenizer(num_words=n_unique_words)\n",
    "tokenizer.fit_on_texts(list(train_sentences_series))\n",
    "train_tokenized_sentences = tokenizer.texts_to_sequences(train_sentences_series)\n",
    "\n",
    "# Tokeninze the Test data\n",
    "test_tokenized_sentences = tokenizer.texts_to_sequences(test_sentences_series)\n",
    "\n",
    "# toxic,severe_toxic,obscene,threat,insult,identity_hate\n",
    "classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y_train = train_df[classes].values\n",
    "\n",
    "X_train = pad_sequences(train_tokenized_sentences, maxlen=max_review_length, padding=pad_type, truncating=trunc_type, value=0)\n",
    "X_test_sub = pad_sequences(test_tokenized_sentences, maxlen=max_review_length, padding=pad_type, truncating=trunc_type, value=0)\n",
    "\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=test_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Design Deep Net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_epsilon = 0.0025\n",
    "dense_epsilon = 0.0083\n",
    "\n",
    "input_layer = Input(shape=(max_review_length,), dtype='int16', name='input')\n",
    "\n",
    "embedding_layer = Embedding(n_unique_words, n_dim, input_length=max_review_length, name='embedding_1')(input_layer)\n",
    "\n",
    "conv_1 = Conv1D(n_conv_1, k_conv_1, name='conv_1')(embedding_layer)\n",
    "act1 = SineReLU(cnn_epsilon)(conv_1)\n",
    "\n",
    "maxp_1 = GlobalMaxPool1D(name='maxp_1')(act1)\n",
    "drop_1 = Dropout(drop_conv)(maxp_1)\n",
    "norm_1 = BatchNormalization()(drop_1)\n",
    "\n",
    "conv_2 = Conv1D(n_conv_2, k_conv_2 name='conv_2')(embedding_layer)\n",
    "act2 = SineReLU(cnn_epsilon)(conv_2)\n",
    "\n",
    "maxp_2 = GlobalMaxPool1D(name='maxp_2')(act2)\n",
    "drop_2 = Dropout(drop_conv)(maxp_2)\n",
    "norm_2 = BatchNormalization()(drop_2)\n",
    "\n",
    "conv_3 = Conv1D(n_conv_3, k_conv_3 name='conv_3')(embedding_layer)\n",
    "act3 = SineReLU(cnn_epsilon)(conv_3)\n",
    "\n",
    "maxp_3 = GlobalMaxPool1D(name='maxp_3')(act3)\n",
    "drop_3 = Dropout(drop_conv)(maxp_3)\n",
    "norm_3 = BatchNormalization()(drop_3)\n",
    "\n",
    "concat = concatenate([norm_1, norm_2, norm_3])\n",
    "\n",
    "dense_layer_1 = Dense(n_dense, name='dense_1')(concat)\n",
    "act4 = SineReLU(dense_epsilon)(dense_layer_1)\n",
    "drop_dense_layer_1 = Dropout(dropout, name='drop_dense_1')(act4)\n",
    "\n",
    "dense_layer_2 = Dense(n_dense, name='dense_2')(drop_dense_layer_1)\n",
    "act5 = SineReLU(dense_epsilon)(dense_layer_2)\n",
    "\n",
    "drop_dense_layer_2 = Dropout(dropout, name='drop_dense_2')(act5)\n",
    "\n",
    "predictions = Dense(n_classes, activation='sigmoid', name='output')(drop_dense_layer_2)\n",
    "\n",
    "model = Model(input_layer, predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCheckpoint = ModelCheckpoint(monitor='val_acc', filepath=output_dir+'/weights-multicnn-toxicity_new.hdf5', save_best_only=True, mode='max')\n",
    "earlyStopping = EarlyStopping(monitor='val_acc', mode='max', patience=patience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1, validation_data=(X_valid, y_valid), callbacks=[modelCheckpoint, earlyStopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_weights(output_dir+'/weights-multicnn-toxicity.hdf5')\n",
    "model = keras.models.load_model(output_dir + '/weights-multicnn-toxicity_new.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = model.predict(X_test_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_hat)\n",
    "_ = plt.axvline(x=0.5, color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(y_hat)\n",
    "pct_auc = roc_auc_score(y_valid, y_hat[0:31915]) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'{:0.2f}'.format(pct_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"kaggle/datasets/toxicity/sample_submission.csv\")\n",
    "\n",
    "sample_submission.shape\n",
    "\n",
    "sample_submission[classes] = y_hat\n",
    "sample_submission.to_csv(\"kaggle/datasets/toxicity/submission_multicnn_relus.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
