{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Convolutional Neural Network in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bulding a Deep Convolutional Neural Network to classify MNIST digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set seed for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten, MaxPooling2D, Conv2D, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import TensorBoard, LearningRateScheduler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 10\n",
    "\n",
    "raw_data = np.loadtxt('kaggle/datasets/train.csv', skiprows=1, dtype='int', delimiter=',')\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    raw_data[:,1:], raw_data[:,0], test_size = 0.1)\n",
    "\n",
    "X_train = X_train.reshape(-1, 28, 28, 1)\n",
    "X_val = X_val.reshape(-1, 28, 28, 1)\n",
    "\n",
    "X_train = X_train.astype(\"float32\")/255.\n",
    "X_val = X_val.astype(\"float32\")/255.\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, n_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, n_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Custom Activation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "def relus(Z):\n",
    "    e_param = 3.8\n",
    "    pi = K.variable((3.14))\n",
    "    m = e_param + (K.sigmoid(K.sin(Z)) - K.sigmoid(K.cos(Z)) * K.exp(K.sqrt(pi)))\n",
    "    A = K.maximum(m, Z)\n",
    "    return A\n",
    "\n",
    "get_custom_objects().update({'ReLU_s': Activation(relus)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Design Neural Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, 4, padding = 'same', activation ='ReLU_s', input_shape = (28, 28, 1)))\n",
    "model.add(Conv2D(16, 4, padding = 'same', activation ='ReLU_s'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(20, 7, padding = 'same', activation ='ReLU_s'))\n",
    "model.add(Conv2D(20, 7, padding = 'same', activation ='ReLU_s'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(28, 2, padding = 'same', activation ='ReLU_s'))\n",
    "model.add(Conv2D(28, 2, activation ='ReLU_s'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation = \"ReLU_s\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 16)        272       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 28, 28, 16)        4112      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 14, 14, 20)        15700     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 14, 14, 20)        19620     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 20)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 7, 7, 20)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 28)          2268      \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 6, 6, 28)          3164      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 3, 3, 28)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 3, 3, 28)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 252)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              259072    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                10250     \n",
      "=================================================================\n",
      "Total params: 314,458\n",
      "Trainable params: 314,458\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(zoom_range = 0.1,\n",
    "                            height_shift_range = 0.1,\n",
    "                            width_shift_range = 0.1,\n",
    "                            rotation_range = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = TensorBoard(\"logs/lenet-cnn-mnist-ReLUs-no-augmentation-30-epochs-new-layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34020 samples, validate on 3780 samples\n",
      "Epoch 1/30\n",
      "34020/34020 [==============================] - 150s - loss: 0.2878 - acc: 0.9044 - val_loss: 0.0682 - val_acc: 0.9791\n",
      "Epoch 2/30\n",
      "34020/34020 [==============================] - 145s - loss: 0.0828 - acc: 0.9756 - val_loss: 0.0499 - val_acc: 0.9862\n",
      "Epoch 3/30\n",
      "34020/34020 [==============================] - 147s - loss: 0.0588 - acc: 0.9824 - val_loss: 0.0332 - val_acc: 0.9910\n",
      "Epoch 4/30\n",
      "34020/34020 [==============================] - 145s - loss: 0.0487 - acc: 0.9855 - val_loss: 0.0332 - val_acc: 0.9905\n",
      "Epoch 5/30\n",
      "34020/34020 [==============================] - 151s - loss: 0.0409 - acc: 0.9883 - val_loss: 0.0284 - val_acc: 0.9910\n",
      "Epoch 6/30\n",
      "34020/34020 [==============================] - 149s - loss: 0.0370 - acc: 0.9894 - val_loss: 0.0270 - val_acc: 0.9929\n",
      "Epoch 7/30\n",
      "34020/34020 [==============================] - 152s - loss: 0.0299 - acc: 0.9907 - val_loss: 0.0328 - val_acc: 0.9915\n",
      "Epoch 8/30\n",
      "34020/34020 [==============================] - 155s - loss: 0.0261 - acc: 0.9919 - val_loss: 0.0264 - val_acc: 0.9918\n",
      "Epoch 9/30\n",
      "34020/34020 [==============================] - 149s - loss: 0.0214 - acc: 0.9940 - val_loss: 0.0273 - val_acc: 0.9915\n",
      "Epoch 10/30\n",
      "34020/34020 [==============================] - 166s - loss: 0.0212 - acc: 0.9936 - val_loss: 0.0279 - val_acc: 0.9907\n",
      "Epoch 11/30\n",
      "34020/34020 [==============================] - 151s - loss: 0.0178 - acc: 0.9947 - val_loss: 0.0216 - val_acc: 0.9934\n",
      "Epoch 12/30\n",
      "34020/34020 [==============================] - 152s - loss: 0.0154 - acc: 0.9952 - val_loss: 0.0264 - val_acc: 0.9929\n",
      "Epoch 13/30\n",
      "34020/34020 [==============================] - 148s - loss: 0.0157 - acc: 0.9954 - val_loss: 0.0197 - val_acc: 0.9931\n",
      "Epoch 14/30\n",
      "34020/34020 [==============================] - 144s - loss: 0.0134 - acc: 0.9957 - val_loss: 0.0209 - val_acc: 0.9947\n",
      "Epoch 15/30\n",
      "34020/34020 [==============================] - 149s - loss: 0.0112 - acc: 0.9968 - val_loss: 0.0254 - val_acc: 0.9931\n",
      "Epoch 16/30\n",
      "34020/34020 [==============================] - 145s - loss: 0.0103 - acc: 0.9965 - val_loss: 0.0221 - val_acc: 0.9939\n",
      "Epoch 17/30\n",
      "34020/34020 [==============================] - 152s - loss: 0.0087 - acc: 0.9974 - val_loss: 0.0228 - val_acc: 0.9934\n",
      "Epoch 18/30\n",
      "34020/34020 [==============================] - 152s - loss: 0.0089 - acc: 0.9968 - val_loss: 0.0211 - val_acc: 0.9939\n",
      "Epoch 19/30\n",
      "34020/34020 [==============================] - 151s - loss: 0.0074 - acc: 0.9978 - val_loss: 0.0225 - val_acc: 0.9926\n",
      "Epoch 20/30\n",
      "34020/34020 [==============================] - 144s - loss: 0.0070 - acc: 0.9980 - val_loss: 0.0187 - val_acc: 0.9944\n",
      "Epoch 21/30\n",
      "34020/34020 [==============================] - 146s - loss: 0.0068 - acc: 0.9981 - val_loss: 0.0196 - val_acc: 0.9947\n",
      "Epoch 22/30\n",
      "34020/34020 [==============================] - 147s - loss: 0.0055 - acc: 0.9982 - val_loss: 0.0190 - val_acc: 0.9947\n",
      "Epoch 23/30\n",
      "34020/34020 [==============================] - 149s - loss: 0.0059 - acc: 0.9982 - val_loss: 0.0203 - val_acc: 0.9942\n",
      "Epoch 24/30\n",
      "34020/34020 [==============================] - 155s - loss: 0.0052 - acc: 0.9984 - val_loss: 0.0191 - val_acc: 0.9952\n",
      "Epoch 25/30\n",
      "34020/34020 [==============================] - 155s - loss: 0.0047 - acc: 0.9984 - val_loss: 0.0171 - val_acc: 0.9952\n",
      "Epoch 26/30\n",
      "34020/34020 [==============================] - 148s - loss: 0.0037 - acc: 0.9986 - val_loss: 0.0197 - val_acc: 0.9955\n",
      "Epoch 27/30\n",
      "34020/34020 [==============================] - 142s - loss: 0.0038 - acc: 0.9989 - val_loss: 0.0209 - val_acc: 0.9944\n",
      "Epoch 28/30\n",
      "34020/34020 [==============================] - 149s - loss: 0.0038 - acc: 0.9988 - val_loss: 0.0196 - val_acc: 0.9950\n",
      "Epoch 29/30\n",
      "34020/34020 [==============================] - 165s - loss: 0.0033 - acc: 0.9989 - val_loss: 0.0200 - val_acc: 0.9947\n",
      "Epoch 30/30\n",
      "34020/34020 [==============================] - 154s - loss: 0.0037 - acc: 0.9989 - val_loss: 0.0202 - val_acc: 0.9942\n"
     ]
    }
   ],
   "source": [
    "# hist = model.fit_generator(datagen.flow(X_train, y_train, batch_size = 64),\n",
    "#                            steps_per_epoch = 300,\n",
    "#                            epochs = 65,\n",
    "#                            verbose = 1,\n",
    "#                            validation_data = (X_val[:400,:], y_val[:400,:]),\n",
    "#                            callbacks = [tensorboard])\n",
    "\n",
    "hist = model.fit(X_train, y_train, batch_size = 32,\n",
    "                           epochs = 30,\n",
    "                           verbose = 1,\n",
    "                           validation_split = 0.1,\n",
    "                           callbacks=[annealer, tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Final Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4192/4200 [============================>.] - ETA: 0sFinal loss: 0.0283, final accuracy: 0.9931\n"
     ]
    }
   ],
   "source": [
    "final_loss, final_acc = model.evaluate(X_val, y_val, verbose = 1)\n",
    "print(\"Final loss: {0:.4f}, final accuracy: {1:.4f}\".format(final_loss, final_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.loadtxt('kaggle/datasets/test.csv', skiprows=1, dtype='int', delimiter=',')\n",
    "X_test = X_test.reshape(28000, 28, 28, 1).astype('float32') / 255.\n",
    "\n",
    "predictions = model.predict(X_test, verbose = 2)\n",
    "predictions = np.argmax(predictions, axis = 1)\n",
    "\n",
    "pd.DataFrame({\"ImageId\": list(range(1, len(predictions) + 1)), \"Label\": predictions}).to_csv('kaggle/results/cnn/submission-ReLUs-no-augmentation-30-epochs-new-layer.csv', index = False, header = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
